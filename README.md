
# StupidAE ‚Äî d8c16 Tiny Patch Autoencoder

StupidAE is a very small, very fast, and intentionally simple model that still works surprisingly well.  
It has **13.24M parameters**, compresses by **8√ó per spatial dimension**, and uses **16 latent channels**.

The main goal: make a AE that doesn‚Äôt slow everything down and is fast enough to run directly during text-to-image training.

---

## Weights

The weights are available on HuggingFace:

üëâ [https://huggingface.co/Muinez/StupidAE](https://huggingface.co/Muinez/StupidAE)

---

## Key Numbers

- Total params: **13,243,539**
- Compression: **d8 (8√ó8 patching)**
- Latent channels: **16 (c16)**
- Training: **30k steps**, batch size **256**, **~3** RTX 5090-hours
- Optimizer: **Muon + SnooC**, LR = `1e-3`
- Trained **without KL loss** (just mse)

---

## Performance (compared to SDXL VAE)

Stats for 1024√ó1024:

| Component | SDXL VAE | StupidAE |
|----------|----------|-----------|
| Encoder FLOPs | 4.34 TFLOPs | **124.18 GFLOPs** |
| Decoder FLOPs | 9.93 TFLOPs | **318.52 GFLOPs** |
| Encoder Params | 34.16M | **~3.8M** |
| Decoder Params | 49.49M | **~9.7M** |

The model is **tens of times faster and lighter**, making it usable directly inside training loops.

---

## Architecture Overview

### ‚ùå No Attention  
It is simply unnecessary for this design and only slows things down.

### üü¶ Encoder  
- Splits the image into **8√ó8 patches**  
- Each patch is encoded **independently**  
- Uses **only 1√ó1 convolutions**  
- Extremely fast

The encoder can handle any aspect ratio, but if you want to mix different ARs inside the same batch, the 1√ó1 conv version becomes inconvenient.
The Linear encoder version solves this completely ‚Äî mixed batches work out of the box, although I haven‚Äôt released it yet ‚Äî I can upload it if needed.

### üü• Decoder  
- Uses standard 3√ó3 convolutions (but 1√ó1 also works with surprisingly few artifacts)  
- Uses a **PixNeRF-style head** instead of stacked upsampling blocks

---

## Limitations

- Reconstruction is not perfect ‚Äî small details may appear slightly blurred.  
- Current MSE loss: 0.0020.  
- This can likely be improved by increasing model size.

---

## Notes on 32√ó Compression

If you want **32√ó spatial compression**, do **not** use naive 32√ó patching ‚Äî quality drops heavily.

A better approach:

1. First stage: patch-8 ‚Üí 16/32 channels  
2. Second stage: patch-4 ‚Üí ~256 channels  

This trains much better and works well for text-to-image training too.  
I‚Äôve tested it, and the results are significantly more stable than naive approaches.

If you want to keep FLOPs low, you could try using patch-16 from the start, but I‚Äôm not sure yet how stable the training would be.

I‚Äôm currently working on a **d32c64** model with reconstruction quality better than Hunyuan VAE, but I‚Äôm limited by compute resources.

---

## Support the Project

I‚Äôm renting an **RTX 5090** and running all experiments on it.  
I‚Äôm currently looking for work and would love to join a team doing text-to-image or video model research.

If you want to support development:

- TRC20: üëâ TPssa5ung2MgqbaVr1aeBQEpHC3xfmm1CL  
- BTC: bc1qfv6pyq5dvs0tths682nhfdnmdwnjvm2av80ej4
- Boosty: https://boosty.to/muinez  

---

## How to use

Here's a minimal example:

```python
import torch
from huggingface_hub import hf_hub_download
from PIL import Image
from torchvision.transforms import v2
from IPython.display import display
import requests
from stae import StupidAE

vae = StupidAE().cuda().half()
vae.load_state_dict(
    torch.load(hf_hub_download(repo_id="Muinez/StupidAE", filename="smol_f8c16.pt"))
)

t = v2.Compose([
    v2.Resize((1024, 1024)),
    v2.ToTensor(),
    v2.Normalize([0.5], [0.5])
])

image = Image.open(requests.get("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG", stream=True).raw).convert("RGB")

with torch.inference_mode():
    image = t(image).unsqueeze(0).cuda().half()
    
    latents = vae.encode(image)
    image_decoded = vae.decode(latents)
    
    image = v2.ToPILImage()(torch.clamp(image_decoded * 0.5 + 0.5, 0, 1).squeeze(0))
    display(image)
```


## Coming Soon

- Linear-encoder variant  
- d32c64 model  
- Tutorial: training text-to-image **without bucketing** (supports mixed aspect ratios)

---

## Cite

```bibtex
@misc{StupidAE,
    title        = {StupidAE: A Tiny Patch-Based Autoencoder for Fast Image Compression},
    author       = {Muinez},
    year         = {2025},
    howpublished = \url{https://github.com/Muinez/StupidAE},
}
```

